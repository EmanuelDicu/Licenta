{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T21:48:38.360071Z",
     "start_time": "2024-08-17T21:48:32.323981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import subprocess\n",
    "from typing import cast\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "from prompts import *\n",
    "\n",
    "deepmind_ds: DatasetDict = cast(DatasetDict, load_dataset(\"deepmind/code_contests\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee03cd6144969f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:24:43.144252Z",
     "start_time": "2024-08-16T05:24:43.124640Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_usaco_tests(problem_id):\n",
    "  test_cases = []\n",
    "  test_folder = f\"datasets/usaco_v3/tests/{problem_id}\"\n",
    "  for i in range(1, 11):\n",
    "    for (input_path, output_path) in [(f\"{test_folder}/I.{i}\", f\"{test_folder}/O.{i}\"), \n",
    "                                      (f\"{test_folder}/{i}.in\", f\"{test_folder}/{i}.out\")]:\n",
    "      if not os.path.isfile(input_path) or not os.path.isfile(output_path):\n",
    "        break\n",
    "      with open(input_path, 'r') as f:\n",
    "        input_data = f.read()\n",
    "      with open(output_path, 'r') as f:\n",
    "        output_data = f.read()\n",
    "      test_cases.append({\n",
    "        \"input\": input_data,\n",
    "        \"output\": output_data\n",
    "      })\n",
    "      \n",
    "  # keep only the smallest 10 test cases\n",
    "  test_cases = sorted(test_cases, key=lambda x: len(x[\"input\"]))[:10]\n",
    "  \n",
    "  return test_cases\n",
    "\n",
    "def parse_usaco_bronze_20_problem_ds():\n",
    "  with open('datasets/usaco_subset307_dict.json', 'r') as f:\n",
    "    usaco_ds = json.load(f)\n",
    "    \n",
    "  # read through the fields of the json\n",
    "  bronze_usaco_20_problem_ds = []\n",
    "  for key in usaco_ds.keys():\n",
    "    # if more than 20 problems, break\n",
    "    if len(bronze_usaco_20_problem_ds) >= 20:\n",
    "      break\n",
    "    problem = usaco_ds[key]\n",
    "    if problem[\"problem_level\"] != \"bronze\":\n",
    "      continue\n",
    "    \n",
    "    # add a new problem to the list, with the following fields: title, description, tests (list of json with input and output fields)\n",
    "    bronze_usaco_20_problem_ds.append({\n",
    "      \"title\": problem[\"name\"],\n",
    "      \"description\": problem[\"description\"],\n",
    "      \"tests\": parse_usaco_tests(problem[\"problem_id\"])\n",
    "    })\n",
    "  \n",
    "  return bronze_usaco_20_problem_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8fb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_deepmind_tests(problem):\n",
    "  test_cases = []\n",
    "  for test_case in problem[\"public_tests\"]:\n",
    "    test_cases.append({\n",
    "      \"input\": test_case[\"input\"],\n",
    "      \"output\": test_case[\"output\"]\n",
    "    })\n",
    "  for test_case in problem[\"generated_tests\"]:\n",
    "    test_cases.append({\n",
    "      \"input\": test_case[\"input\"],\n",
    "      \"output\": test_case[\"output\"]\n",
    "    })\n",
    "    \n",
    "  # keep only the smallest 10 test cases\n",
    "  test_cases = sorted(test_cases, key=lambda x: len(x[\"input\"]))[:10]\n",
    "  \n",
    "  return test_cases\n",
    "\n",
    "def parse_deepmind_20_1400rating_ds():\n",
    "  deepmind_20_1400rating_ds = []\n",
    "  for problem in cast(dict, deepmind_ds['train']):\n",
    "    if len(deepmind_20_1400rating_ds) >= 20:\n",
    "      break\n",
    "    if problem[\"cf_rating\"] > 1400:\n",
    "      continue\n",
    "    deepmind_20_1400rating_ds.append({\n",
    "      \"title\": problem[\"title\"],\n",
    "      \"description\": problem[\"description\"],\n",
    "      \"tests\": parse_deepmind_tests(problem)\n",
    "    })\n",
    "    \n",
    "  return deepmind_20_1400rating_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec37774c7ced0e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:24:09.916642Z",
     "start_time": "2024-08-16T05:24:09.903634Z"
    }
   },
   "outputs": [],
   "source": [
    "class CompilationError():\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "\n",
    "class CompilationSuccess():\n",
    "  pass\n",
    "\n",
    "class RuntimeError():\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "\n",
    "class RunSuccess():\n",
    "  def __init__(self, output):\n",
    "    self.output = output\n",
    "\n",
    "class TestFailed():\n",
    "  def __init__(self, message, input, expected_output, output, number_of_tests, tests_passed):\n",
    "    self.message = message\n",
    "    self.input = input\n",
    "    self.expected_output = expected_output\n",
    "    self.output = output\n",
    "    self.number_of_tests = number_of_tests\n",
    "    self.tests_passed = tests_passed\n",
    "\n",
    "class TestSuccess():\n",
    "  def __init__(self, message, number_of_tests):\n",
    "    self.message = message\n",
    "    self.number_of_tests = number_of_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b3fe2f8d4ceec90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:24:16.239002Z",
     "start_time": "2024-08-16T05:24:16.235872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to compile the C++ program\n",
    "def compile_cpp(solution_code, task_name, strategy):\n",
    "  folder = f\"generated/{strategy}/{task_name}\"\n",
    "  os.makedirs(folder, exist_ok=True)\n",
    "  for file in os.listdir(folder):\n",
    "    os.remove(f\"{folder}/{file}\")\n",
    "    \n",
    "  with open(f\"{folder}/{task_name}.cpp\", \"w\") as f:\n",
    "    f.write(solution_code)\n",
    "    compile_command = [\"g++\", f\"{folder}/{task_name}.cpp\", \"-o\", f\"{folder}/{task_name}\"]\n",
    "\n",
    "  result = subprocess.run(compile_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  if result.returncode != 0:\n",
    "    with open(f\"{folder}/compilation_error.txt\", \"w\") as f:\n",
    "      f.write(result.stderr.decode('utf-8'))\n",
    "\n",
    "    return CompilationError(f\"Compilation failed: {result.stderr.decode('utf-8')}\")\n",
    "  return CompilationSuccess()\n",
    "\n",
    "def get_compilation_report(content, messages, problem, strategy):\n",
    "  start = content.find(\"cpp{{\")\n",
    "  end = content.find(\"}}cpp\")\n",
    "\n",
    "  if start == -1 or end == -1:\n",
    "    messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"You MUST provide the solution code in the format of cpp{{YOUR SOLUTION HERE}}cpp.\"\n",
    "    })\n",
    "    return None\n",
    "\n",
    "  solution_code = content[start+5:end]\n",
    "  task_name = problem[\"title\"]\n",
    "  \n",
    "  r = compile_cpp(solution_code, task_name, strategy)\n",
    "  if r.__class__.__name__ == \"CompilationError\":\n",
    "    messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Compilation failed. Please correct the errors and try again.\\\n",
    "                  \\n\\nError message: {}\".format(r.message)\n",
    "    })\n",
    "    return None\n",
    "\n",
    "  return solution_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11a41b7c03aad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemOutput:\n",
    "  def __init__(self, passed_tests, rte_tests, failed_tests, solution_code):\n",
    "    self.passed_tests = passed_tests\n",
    "    self.rte_tests = rte_tests\n",
    "    self.failed_tests = failed_tests\n",
    "    self.solution_code = solution_code\n",
    "    \n",
    "def run_program(executable, input_data):\n",
    "  process = subprocess.Popen([f\"./{executable}\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  output, error = process.communicate(input=input_data.encode())\n",
    "  if process.returncode != 0:\n",
    "    return RuntimeError(f\"Runtime error: {error.decode('utf-8')}\")\n",
    "  return RunSuccess(output.decode('utf-8'))\n",
    "\n",
    "def run_tests(test_cases, executable):\n",
    "  f = open('tmp_output.txt', 'w')\n",
    "  for i, test in enumerate(test_cases):\n",
    "    input_data = test[\"input\"]\n",
    "    expected_output = test[\"output\"]\n",
    "    output = run_program(executable, input_data)\n",
    "    if output.__class__.__name__ != \"RunSuccess\":\n",
    "      return output\n",
    "    \n",
    "    actual_output = output.output\n",
    "\n",
    "    # Compare outputs\n",
    "    if actual_output.strip() == expected_output.strip():\n",
    "      f.write(f\"Test case {i+1}: PASSED\\n\")\n",
    "    else:\n",
    "      f.write(f\"Test case {i+1}: FAILED\\n\")\n",
    "      f.write(f\"Expected output:\\n{expected_output}\\n\")\n",
    "      f.write(f\"Actual output:\\n{actual_output}\\n\")\n",
    "      return TestFailed(f\"Test case {i+1} failed\", input_data, expected_output, actual_output, len(test_cases), i+1)\n",
    "  \n",
    "  f.close()\n",
    "  os.remove('tmp_output.txt')\n",
    "  \n",
    "  return TestSuccess(\"All test cases passed\", len(test_cases)) \n",
    "\n",
    "def get_runtime_report(problem, strategy, solution_code):\n",
    "  task_name = problem[\"title\"]\n",
    "  \n",
    "  passed_tests = []\n",
    "  rte_tests = []\n",
    "  failed_tests = []\n",
    "\n",
    "  for test_case in problem[\"tests\"]:\n",
    "    input_data = test_case[\"input\"]\n",
    "    expected_output = test_case[\"output\"]\n",
    "\n",
    "    output = run_program(f\"generated/{strategy}/{task_name}/{task_name}\", input_data)\n",
    "    if output.__class__.__name__ == \"RuntimeError\":\n",
    "      rte_tests.append({\n",
    "        \"input\": input_data,\n",
    "        \"output\": expected_output,\n",
    "        \"error\": output.message\n",
    "      })\n",
    "    elif output.__class__.__name__ == \"RunSuccess\" and output.output.strip() == expected_output.strip():\n",
    "      passed_tests.append({\n",
    "        \"input\": input_data,\n",
    "        \"output\": expected_output\n",
    "      })\n",
    "    else:\n",
    "      failed_tests.append({\n",
    "        \"input\": input_data,\n",
    "        \"expected_output\": expected_output,\n",
    "        \"actual_output\": output.output\n",
    "      })\n",
    "      \n",
    "  return ProblemOutput(passed_tests, rte_tests, failed_tests, solution_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56256c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='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')\n",
    "\n",
    "def call_ai(model, messages):\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    "  )\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "  time.sleep(20)\n",
    "  content = response.choices[0].message.content\n",
    "  \n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d893f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpp_code(model, messages, problem, strategy):\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  best_output = None\n",
    "  trials_since_last_solution = 0\n",
    "\n",
    "  for _ in range(10):\n",
    "    source_code = get_compilation_report(content, messages, problem, strategy)\n",
    "    if source_code is None:\n",
    "      continue\n",
    "\n",
    "    run_report = get_runtime_report(problem, strategy, source_code)\n",
    "    if best_output is None or len(run_report.passed_tests) > len(best_output.passed_tests):\n",
    "      best_output = run_report\n",
    "      trials_since_last_solution = 0\n",
    "      \n",
    "      if len(run_report.passed_tests) == len(problem[\"tests\"]):\n",
    "        break\n",
    "    else:\n",
    "      trials_since_last_solution += 1\n",
    "      if trials_since_last_solution >= 4:\n",
    "        messages.append({\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"You MUST start again from your last best solution and try to improve it by fixing the failed tests.\\\n",
    "                      \\nYour last best solution is:\\n\\n{}\".format(best_output.solution_code)\n",
    "        })\n",
    "    \n",
    "  return best_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c9f8e5d1da415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solution_direct_prompting(problem, model):\n",
    "    strategy = \"direct_prompting\"\n",
    "    title = problem[\"title\"]\n",
    "    description = problem[\"description\"]\n",
    "    public_test = problem[\"test_cases\"][0]\n",
    "\n",
    "    messages = [{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": generate_solution_direct_prompt.format(Problem=title, Objective=description, Input=public_test[\"input\"], Output=public_test[\"output\"])\n",
    "    }]\n",
    "\n",
    "    return get_cpp_code(model, messages, problem, strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00dca682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_and_constraints(model, problem):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": get_objective_and_constraints_prompt.format(Problem=problem[\"description\"])\n",
    "  }]\n",
    "\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  # find constraints\n",
    "  constraints = []\n",
    "  start = 0\n",
    "  while True:\n",
    "    start = content.find(\"cns{{\", start)\n",
    "    if start == -1:\n",
    "      break\n",
    "    end = content.find(\"}}cns\", start)\n",
    "    constraints.append(content[start+5:end])\n",
    "    start = end\n",
    "  \n",
    "  # find objective\n",
    "  start = content.find(\"obj{{\")\n",
    "  end = content.find(\"}}obj\")\n",
    "  objective = content[start+5:end]\n",
    "\n",
    "  return constraints, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d13597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_constraint(model, problem, constraints, objective):\n",
    "  messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": get_additional_constraint_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in constraints]), Objective=\"- \" + objective)\n",
    "  }]\n",
    "\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  # find additional constraint\n",
    "  start = content.find(\"ncns{{\")\n",
    "  end = content.find(\"}}ncns\")\n",
    "  additional_constraint = content[start+6:end]\n",
    "\n",
    "  return additional_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec2049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_constraint(problem, constraints, additional_constraint):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": test_constraint_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in constraints]), Objective=\"- \" + problem[\"goal\"], NewConstraint=additional_constraint)\n",
    "  }]\n",
    "\n",
    "  cnt_yes = 0\n",
    "  for model in [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-3.5-turbo-0125\"]:\n",
    "    content = call_ai(model, messages)\n",
    "    start = content.find(\"Answer:\")\n",
    "    end = content.find(\"```\", start)\n",
    "    answer = content[start+8:end].strip()\n",
    "    if answer == \"YES\":\n",
    "      cnt_yes += 1\n",
    "    if cnt_yes == 2:\n",
    "      return True\n",
    "    \n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2edda9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_constraint(model, problem, constraints, objective, incorrect_constraint):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": fix_constraint_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in constraints]), Objective=\"- \" + objective, Incorrect_constraint=incorrect_constraint)\n",
    "  }]\n",
    "\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  # find fixed constraint in the content\n",
    "  start = content.find(\"Corrected constraint:\")\n",
    "  end = content.find(\"```\", start)\n",
    "  fixed_constraint = content[start+21:end].strip()\n",
    "\n",
    "  return fixed_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83a17112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_solve_problem(model, problem, constraints, objective):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": can_solve_problem_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in constraints]), Objective=\"- \" + objective)\n",
    "  }]\n",
    "\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  # find answer\n",
    "  start = content.find(\"Answer:\")\n",
    "  end = content.find(\"```\", start)\n",
    "  answer = content[start+8:end].strip()\n",
    "\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad366672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solution_MACM(problem, model):\n",
    "  strategy = \"MACM\"\n",
    "\n",
    "  constraints, objective = get_objective_and_constraints(model, problem)\n",
    "  for _ in range(10):\n",
    "    additional_constraint = get_additional_constraint(model, problem, constraints, objective)\n",
    "    if not is_valid_constraint(problem, constraints, additional_constraint):\n",
    "      additional_constraint = fix_constraint(model, problem, strategy, constraints, additional_constraint)\n",
    "      if additional_constraint is not None:\n",
    "        constraints.append(additional_constraint)\n",
    "    if can_solve_problem(model, problem, constraints, objective):\n",
    "      break\n",
    "\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": generate_solution_MACM_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in constraints]), Objective=\"- \" + objective, Input=problem[\"input\"], Output=problem[\"output\"])\n",
    "  }]\n",
    "\n",
    "  return get_cpp_code(model, messages, problem, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf634513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solution_flow_engineering(problem, model):\n",
    "  strategy = \"flow_engineering\"\n",
    "\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": explain_bullet_points_prompt.format(Problem=problem[\"description\"])\n",
    "  }]\n",
    "  call_ai(model, messages)\n",
    "\n",
    "  messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": explain_input_output_prompt.format(\n",
    "      Input1=problem[\"test_cases\"][0][\"input\"], Output1=problem[\"test_cases\"][0][\"output\"],\n",
    "      Input2=problem[\"test_cases\"][1][\"input\"], Output2=problem[\"test_cases\"][1][\"output\"],\n",
    "      Input3=problem[\"test_cases\"][2][\"input\"], Output3=problem[\"test_cases\"][2][\"output\"],\n",
    "    )\n",
    "  })\n",
    "  call_ai(model, messages)\n",
    "\n",
    "  messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": generate_starting_solutions_prompt\n",
    "  })\n",
    "  call_ai(model, messages)\n",
    "\n",
    "  messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": gen_final_solution_flow_engineering_prompt\n",
    "  })\n",
    "  return get_cpp_code(model, messages, problem, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ce55e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_for_problem(model, problem):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": get_problem_objective_prompt.format(Problem=problem[\"description\"])\n",
    "  }]\n",
    "  content = call_ai(model, messages)\n",
    "  \n",
    "  start = content.find(\"obj{{\")\n",
    "  end = content.find(\"}}obj\")\n",
    "  objective = content[start+5:end]\n",
    "\n",
    "  return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "054e297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_key_point(model, problem, objective, key_points):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": get_new_key_point_prompt.format(Problem=problem[\"description\"], \n",
    "      Objective=objective, Key_points=\"\\n\".join([\"- \" + kp for kp in key_points]))\n",
    "  }]\n",
    "  content = call_ai(model, messages)\n",
    "  \n",
    "  start = content.find(\"key{{\")\n",
    "  end = content.find(\"}}key\")\n",
    "  new_key_point = content[start+5:end]\n",
    "\n",
    "  return new_key_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc81fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_likelyhood_of_key_point(model, problem, objective, key_points, eval_key_point):\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": validate_likelihood_of_key_point_prompt.format(Problem=problem[\"description\"], \n",
    "      Objective=objective, Key_points=\"\\n\".join([\"- \" + kp for kp in key_points]), Eval_key_point=eval_key_point)\n",
    "  }]    \n",
    "\n",
    "  content = call_ai(model, messages)\n",
    "\n",
    "  start = content.find(\"Answer:\")\n",
    "  end = content.find(\"```\", start)\n",
    "  answer = content[start+8:end].strip()\n",
    "\n",
    "  return answer\n",
    "\n",
    "def get_node_score(model, problem, objective, key_points, eval_key_point):\n",
    "\n",
    "  score = 0.0\n",
    "  for _ in range(3):\n",
    "    answer = validate_likelyhood_of_key_point(model, problem, objective, key_points, eval_key_point)\n",
    "    if answer == \"SURE\":\n",
    "      score += 1.0\n",
    "    elif answer == \"LIKELY\":\n",
    "      score += 0.5\n",
    "    elif answer == \"UNLIKELY\":\n",
    "      score -= 0.5\n",
    "    elif answer == \"NO\":\n",
    "      score -= 1.0\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5301b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThoughtNode:\n",
    "  def __init__(self, key_points):\n",
    "    self.key_points = key_points\n",
    "\n",
    "def get_solution_tree_of_thoughts(problem, model):\n",
    "  strategy = \"ToT\"\n",
    "  get_solution_ToT_prompt = generate_solution_MACM_prompt\n",
    "  objective = get_objective_for_problem(model, problem)\n",
    "  current_level_nodes = [ThoughtNode([])]\n",
    "\n",
    "  for _ in range(3):\n",
    "    new_level_nodes = []\n",
    "    for node in current_level_nodes:\n",
    "      for _ in range(3):\n",
    "        new_key_point = get_new_key_point(model, problem, objective, node.key_points)\n",
    "        new_level_nodes.append((new_key_point, ThoughtNode(node.key_points + [new_key_point])))\n",
    "\n",
    "    current_level_nodes = new_level_nodes.sort(key=lambda x: get_node_score(model, problem, objective, x[1].key_points, x[0]), reverse=True)[:5]\n",
    "\n",
    "  key_points = []\n",
    "  for node in current_level_nodes:\n",
    "    key_points += node.key_points\n",
    "\n",
    "  messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": get_solution_ToT_prompt.format(Problem=problem[\"description\"], \n",
    "      Constraints=\"\\n\".join([\"- \" + c for c in key_points]), Objective=\"- \" + objective, Input=problem[\"input\"], Output=problem[\"output\"])\n",
    "  }]\n",
    "\n",
    "  return get_cpp_code(model, messages, problem, strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
